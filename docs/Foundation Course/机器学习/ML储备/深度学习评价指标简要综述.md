[深度学习评价指标简要综述 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/188671935)

## 一、图像分类评测指标

### 1.TP、TN、FP、FN

-   TP（True Positives）我们**倒着来翻译**就是“预测为正样本，并且预测对了”（真阳性）
-   TN（True Negatives）意思是“预测为负样本，而且预测对了”（真阴性）
-   FP（False Positives）意思是“预测为正样本，但是预测错了”（假阳性）
-   FN（False Negatives）意思是“预测为负样本，但是预测错了”（假阴性）

![](https://pic2.zhimg.com/v2-a8f4022453c7cf87ea4316b38d076521_b.jpg)

### 2. 混淆矩阵

如果我们想知道**类别之间相互误分的情况，查看是否有特定的类别相互混淆**，就可以用混淆矩阵画出分类的详细预测结果。对于包含多个类别的任务，可以很清晰的反映各类别之间的错分概率。

比如我们一个模型对15个样本进行预测，然后结果如下：<sup data-text="" data-url="http://sofasofa.io/forum_main_post.php?postid=1000597" data-numero="1" data-draft-node="inline" data-draft-type="reference" data-tooltip="http://sofasofa.io/forum_main_post.php?postid=1000597" data-tooltip-preset="white" data-tooltip-classname="ztext-referene-tooltip"><a id="ref_1_0" href="https://zhuanlan.zhihu.com/p/188671935#ref_1" data-reference-link="true" aria-labelledby="ref_1" pcked="1">[1]</a></sup>

-   预测值：1 1 1 1 1 0 0 0 0 0 1 1 1 0 1
-   真实值：0 1 1 0 1 1 0 0 1 0 1 0 1 0 0

![](https://pic1.zhimg.com/v2-f903b6de230c27a7b9d5e6a47642c784_b.jpg)

混淆矩阵

### 3.准确率(Accuracy)、精确率(Precision)、召回率(Recall)、F1-score

准确率(Accuracy)、精确率(Precision)、召回率(Recall)、F1-score<sup data-text="代码参考(有修改)" data-url="https://github.com/LeeJunHyun/Image_Segmentation/blob/master/evaluation.py" data-numero="2" data-draft-node="inline" data-draft-type="reference" data-tooltip="代码参考(有修改) https://github.com/LeeJunHyun/Image_Segmentation/blob/master/evaluation.py" data-tooltip-preset="white" data-tooltip-classname="ztext-referene-tooltip"><a id="ref_2_0" href="https://zhuanlan.zhihu.com/p/188671935#ref_2" data-reference-link="true" aria-labelledby="ref_2" pcked="1">[2]</a></sup>

$\mathrm{Accuracy}=\frac{T P+T N}{P+N}$\\mathrm{Accuracy}=\\frac{T P+T N}{P+N} _(所有预测正确的正例和负例，占所有样本的比例)_

```
# 代码为torch版本
# 其中SR为预测的mask，即Segmentation Result
# 其中GT为真实的标注，即Ground Truth
def get_accuracy(SR,GT,threshold=0.5):
    SR = SR > threshold
    GT = GT == torch.max(GT)
    corr = torch.sum(SR==GT)
    tensor_size = SR.size(0)*SR.size(1)*SR.size(2)*SR.size(3)
    acc = float(corr)/float(tensor_size)

    return acc
```

$Precision =\frac{T P}{T P+F P}$Precision =\\frac{T P}{T P+F P} _(预测为正例并且确实是正例的部分，占所有预测为正例的比例)_

```
# 代码为torch版本
# 其中SR为预测的mask，即Segmentation Result
# 其中GT为真实的标注，即Ground Truth
def get_precision(SR, GT, threshold=0.5):
    SR = SR > threshold
    GT = GT == torch.max(GT)
    TP = ((SR == 1) & (GT == 1))
    FP = ((SR == 1) & (GT == 0))
    PC = float(torch.sum(TP)) / (float(torch.sum(TP + FP)) + 1e-6)

    return PC
```

$Recall =\frac{T P}{T P+F N}$Recall =\\frac{T P}{T P+F N}_(预测为正例并且确实是正例的部分，占所有确实是正类的比例)_

$F 1-s c o r e=\frac{2}{\frac{1}{\text { Precision }}+\frac{1}{\text { Recall }}}=\frac{2 T P}{F P+2 T P+F N}$F 1-s c o r e=\\frac{2}{\\frac{1}{\\text { Precision }}+\\frac{1}{\\text { Recall }}}=\\frac{2 T P}{F P+2 T P+F N}

_(精确率与召回率的调和平均数，计算公式同Dice相似系数)_

```
# 代码为torch版本
# 其中SR为预测的mask，即Segmentation Result
# 其中GT为真实的标注，即Ground Truth
def get_F1(SR, GT, threshold=0.5):
    SE = get_sensitivity(SR, GT, threshold=threshold)
    PC = get_precision(SR, GT, threshold=threshold)
    F1 = 2 * SE * PC / (SE + PC + 1e-6)

    return F1
```

### 4.灵敏度(Sensitivity)、特异度(Specificity)

灵敏度(Sensitivity)、特异度(Specificity)<sup data-text="" data-url="https://segmentfault.com/a/1190000016686335" data-numero="3" data-draft-node="inline" data-draft-type="reference" data-tooltip="https://segmentfault.com/a/1190000016686335" data-tooltip-preset="white" data-tooltip-classname="ztext-referene-tooltip"><a id="ref_3_0" href="https://zhuanlan.zhihu.com/p/188671935#ref_3" data-reference-link="true" aria-labelledby="ref_3" pcked="1">[3]</a></sup>

$Sensitivity =\frac{T P}{T P+F N}=Recall$Sensitivity =\\frac{T P}{T P+F N}=Recall _(灵敏度，同召回率)_

```
# 代码为torch版本
# 其中SR为预测的mask，即Segmentation Result
# 其中GT为真实的标注，即Ground Truth
# Sensitivity == Recall
def get_sensitivity(SR, GT, threshold=0.5):
    SR = SR > threshold
    GT = GT == torch.max(GT)
    TP = ((SR == 1) & (GT == 1))
    FN = ((SR == 0) & (GT == 1))
    SE = float(torch.sum(TP)) / (float(torch.sum(TP + FN)) + 1e-6)

    return SE
```

$Specificity =\frac{T N}{T N+F P}$Specificity =\\frac{T N}{T N+F P} _(预测为负例并且确实是负例的部分，占所有确实为负例的比例)_

```
# 代码为torch版本
# 其中SR为预测的mask，即Segmentation Result
# 其中GT为真实的标注，即Ground Truth
def get_specificity(SR, GT, threshold=0.5):
    SR = SR > threshold
    GT = GT == torch.max(GT)
    TN = ((SR == 0) & (GT == 0))
    FP = ((SR == 1) & (GT == 0))
    SP = float(torch.sum(TN)) / (float(torch.sum(TN + FP)) + 1e-6)

    return SP
```

### 5. 真正例率(TPR)、假正例率(FPR)

$T P R=\frac{T P}{T P+F N}=$T P R=\\frac{T P}{T P+F N}= 灵敏度

_(真正例率，同灵敏度)_

$F P R=\frac{F P}{F P+T N}=$F P R=\\frac{F P}{F P+T N}= 1-特异度

_(我们比较关心正样本，要查看有多少负样本被错误的预测为正样本)_

### 6. ROC曲线、AUC

**ROC** (Receiver Operating Characteristic) 曲线，又称接受者操作特征曲线。该曲线最早应用于雷达信号检测领域，用于区分信号与噪声，ROC曲线是基于混淆矩阵得出的。**横坐标为假正率(FPR)，纵坐标为真正率(TPR)。**

**TPR越高，同时FPR越低（即ROC曲线越陡），那么模型的性能就越好。**

![](https://pic4.zhimg.com/v2-eb34f7e3dd67783285b987a8b6c5974f_b.jpg)

标准ROC曲线

**AUC**（Area Under Curve），也就是**ROC曲线下的面积。**

```
# python 代码示例
from sklearn import metrics 
from sklearn.metrics import auc
import numpy as np 
y = np.array([1,1,2,2])
scores = np.array ([0.1,0.4,0.35,0.8])
fpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=2) 
metrics.auc(fpr, tpr)
```

### 7.人脸识别算法评价指标

人脸识别算法评价指标<sup data-text="" data-url="https://blog.csdn.net/weixin_38208741/article/details/78547248" data-numero="4" data-draft-node="inline" data-draft-type="reference" data-tooltip="https://blog.csdn.net/weixin_38208741/article/details/78547248" data-tooltip-preset="white" data-tooltip-classname="ztext-referene-tooltip"><a id="ref_4_0" href="https://zhuanlan.zhihu.com/p/188671935#ref_4" data-reference-link="true" aria-labelledby="ref_4" pcked="1">[4]</a></sup>

**TAR**（True Accept Rate）表示**正确接受的比例**<sup data-text="" data-url="https://zhuanlan.zhihu.com/p/87503403" data-numero="5" data-draft-node="inline" data-draft-type="reference" data-tooltip="https://zhuanlan.zhihu.com/p/87503403" data-tooltip-preset="white" data-tooltip-classname="ztext-referene-tooltip"><a id="ref_5_0" href="https://zhuanlan.zhihu.com/p/188671935#ref_5" data-reference-link="true" aria-labelledby="ref_5" pcked="1">[5]</a></sup>

![](https://pic3.zhimg.com/v2-15ba8b2dbf0df240d75e721f3d4ba9ee_b.jpg)

对属于同一人的图像对进行比较，把它们当作是同一人的图像的比例。T为阈值，TAR越大越好。

**FAR**（False Acceptance Rate）误识率

![](https://pic1.zhimg.com/v2-24df6318ae62a359f6c1b8a69013cb38_b.jpg)

我们比较不同人的图象时，把其中的图像对当成同一个人图像的比例。FAR越小越好。

**FRR**（False Rejection Rate）拒识率

![](https://pic2.zhimg.com/v2-76606c0321e7574028d8b8f353d2a23d_b.jpg)

FRR就是把相同的人的图像对，当成不同的人的了。

> 总结可得，$\mathrm{FRR}+\mathrm{TAR}=1 , T A R=1-F R R$ \\mathrm{FRR}+\\mathrm{TAR}=1 , T A R=1-F R R

**EER**（Equal Error Rate）等误率

也就是取某个T值时，使得FAR=FRR ，此时的FAR或FRR值。

## 二、检索与回归评测指标

### 1. IoU（Intersection over Union）

即交并比，IoU 计算的是 “预测的边框” 和 “真实的边框” 的**交集和并集的比值**。

> 在图像分割中这一指标通常比直接计算每个像素的分类正确概率要低，也对错误分类更加敏感。

![](https://pic3.zhimg.com/v2-52f0825d26ffcbac0b21d74c22be004e_b.jpg)

```
# 代码为torch版本
# 其中SR为预测的mask，即Segmentation Result
# 其中GT为真实的标注，即Ground Truth
def get_IOU(SR, GT, threshold=0.5):
    SR = SR > threshold  
    GT = GT == torch.max(GT)
    TP = ((SR == 1) & (GT == 1))
    FP = ((SR == 1) & (GT == 0))
    FN = ((SR == 0) & (GT == 1))
    IOU = float(torch.sum(TP)) / (float(torch.sum(TP + FP + FN)) + 1e-6)

    return IOU
```

### 2. AP、mAP

[轻松计算目标检测评价指标mAP - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/139073511)

**mAP（均值平均精度）**是目标检测中的最常用的评价指标，详细理解其计算方式有助于我们评估算法的有效性，并针对评测指标对算法进行调整。

#### 举例计算mAP

有3张图如下，要求算法找出face。蓝色框代表标签label，绿色框代表算法给出的结果pre，旁边的红色小字代表置信度。

设定第一张图的预测框叫pre1，第一张的真实框叫label1。第二张、第三张同理。

![](https://pic1.zhimg.com/v2-2fbf0828ab789605f5a73cdd18d680ac_b.jpg)

![](https://pic2.zhimg.com/v2-cb70e4ad4cb7533d81c9c588616069f5_b.jpg)

##### 1.根据IOU计算TP,FP

首先我们计算每张图的pre和label的IOU，根据IOU是否大于0.5来判断该pre是属于TP还是属于FP。显而易见，pre1是TP，pre2是FP，pre3是TP。

##### 2.置信度排序

根据每个pre的置信度进行从高到低排序，这里pre1、pre2、pre3置信度刚好就是从高到低。

##### 3.在不同置信度阈值下获得Precision和Recall

-   首先，设置阈值为0.9，无视所有小于0.9的pre。那么检测器检出的所有框pre即TP+FP=1，并且pre1是TP，那么Precision=1/1。因为所有的label=3，所以Recall=1/3。这样就得到一组P、R值。
-   然后，设置阈值为0.8，无视所有小于0.8的pre。那么检测器检出的所有框pre即TP+FP=2，因为pre1是TP，pre2是FP，那么Precision=1/2=0.5。因为所有的label=3，所以Recall=1/3=0.33。这样就又得到一组P、R值。
-   再然后，设置阈值为0.7，无视所有小于0.7的pre。那么检测器检出的所有框pre即TP+FP=3，因为pre1是TP，pre2是FP，pre3是TP，那么Precision=2/3=0.67。因为所有的label=3，所以Recall=2/3=0.67。这样就又得到一组P、R值。

##### 4.绘制PR曲线并计算AP值

根据上面3组PR值绘制PR曲线如下。然后每个“峰值点”往左画一条线段直到与上一个峰值点的垂直线相交。这样画出来的红色线段与坐标轴围起来的面积就是AP值。

![](https://pic4.zhimg.com/v2-4bca2079016dce0eab97dded6ca458cb_b.jpg)

![](https://pic3.zhimg.com/v2-8a606e9830bb4ecf0a2cee8c5dce3e8e_b.jpg)

##### 5.计算mAP

计算mAP<sup data-text="" data-url="https://www.cnblogs.com/Tom-Ren/p/11054605.html" data-numero="2" data-draft-node="inline" data-draft-type="reference" data-tooltip="https://www.cnblogs.com/Tom-Ren/p/11054605.html" data-tooltip-preset="white" data-tooltip-classname="ztext-referene-tooltip"><a id="ref_2_0" href="https://zhuanlan.zhihu.com/p/139073511#ref_2" data-reference-link="true" aria-labelledby="ref_2" pcked="1">[2]</a></sup>

AP衡量的是对一个类检测好坏，mAP就是对多个类的检测好坏。就是简单粗暴的把所有类的AP值取平均就好了。比如有两类，类A的AP值是0.5，类B的AP值是0.2，那么mAP=（0.5+0.2）/2=0.35

```
# AP的计算
def _average_precision(self, rec, prec):
    """
    Params:
    ----------
    rec : numpy.array
            cumulated recall
    prec : numpy.array
            cumulated precision
    Returns:
    ----------
    ap as float
    """
    if rec is None or prec is None:
        return np.nan
    ap = 0.
    for t in np.arange(0., 1.1, 0.1):  #十一个点的召回率，对应精度最大值
        if np.sum(rec >= t) == 0:
            p = 0
        else:
            p = np.max(np.nan_to_num(prec)[rec >= t])
        ap += p / 11.  #加权平均
    return ap
```

##### 参考

1.  [^](https://zhuanlan.zhihu.com/p/139073511#ref_1_0)[https://blog.csdn.net/hsqyc/article/details/81702437#comments\_10227172](https://blog.csdn.net/hsqyc/article/details/81702437#comments_10227172)
2.  [^](https://zhuanlan.zhihu.com/p/139073511#ref_2_0)[https://www.cnblogs.com/Tom-Ren/p/11054605.html](https://www.cnblogs.com/Tom-Ren/p/11054605.html)

## 三、图像分割评测指标

[图像分割评测指标](https://zhuanlan.zhihu.com/p/159173338)

### 1.Dice相似系数

一种集合相似度度量指标，通常用于计算两个样本的相似度，值的范围0~1，分割结果最好时值为1，最差时值为0。**对mask的内部填充比较敏感**。

![](https://pic2.zhimg.com/v2-7d60c2eb2268759f2e1f5635effea829_b.jpg)

Dice

$\operatorname{Dice}(P, T)=\frac{\left|P_{1} \wedge T_{1}\right|}{\left(\left|P_{1}\right|+\left|T_{2}\right|\right) / 2} \Leftrightarrow \text { Dice }=\frac{2 T P}{F P+2 T P+F N}$\\operatorname{Dice}(P, T)=\\frac{\\left|P\_{1} \\wedge T\_{1}\\right|}{\\left(\\left|P\_{1}\\right|+\\left|T\_{2}\\right|\\right) / 2} \\Leftrightarrow \\text { Dice }=\\frac{2 T P}{F P+2 T P+F N}

```
# 代码为torch版本
# 其中SR为预测的mask，即Segmentation Result
# 其中GT为真实的标注，即Ground Truth
def get_DC(SR, GT, threshold=0.5):
    # DC : Dice Coefficient
    SR = SR > threshold
    GT = GT == torch.max(GT)
    Inter = torch.sum(SR & GT)
    DC = float(2 * Inter) / (float(torch.sum(SR) + torch.sum(GT)) + 1e-6)

    return DC
```

### 2. Hausdorff距离

描述两组点集之间相似程度的一种量度，**对分割出的边界比较敏感。**

![](https://pic3.zhimg.com/v2-d74fbc7430fb55ede056ea83936415de_b.jpg)

Hd

$d_{H}(X, Y)=\max \left|d_{X Y}, d_{Y X}\right|=\max \left\{\max _{x \in X} \min _{y \in Y} d(x, y), \max _{y \in Y} \min _{x \in X} d|x, y|\right\}$d\_{H}(X, Y)=\\max \\left|d\_{X Y}, d\_{Y X}\\right|=\\max \\left\\{\\max \_{x \\in X} \\min \_{y \\in Y} d(x, y), \\max \_{y \\in Y} \\min \_{x \\in X} d|x, y|\\right\\}

## 四、图像生成评测指标

### 1. Inception Score

最早用于GAN生成图像多样性评估的指标，利用了Google的Inception模型来进行评估。

### 2. 最大平均差异（_Maximum Mean Discrepancy_）

也是一个用于判断两个分布 p 和 q 是否相同的指标。

### 3. Wasserstein距离

### 4. Frechet Inception距离

## 参考

1.  [^](https://zhuanlan.zhihu.com/p/188671935#ref_1_0)[http://sofasofa.io/forum\_main\_post.php?postid=1000597](http://sofasofa.io/forum_main_post.php?postid=1000597)
2.  [^](https://zhuanlan.zhihu.com/p/188671935#ref_2_0)代码参考(有修改) [https://github.com/LeeJunHyun/Image\_Segmentation/blob/master/evaluation.py](https://github.com/LeeJunHyun/Image_Segmentation/blob/master/evaluation.py)
3.  [^](https://zhuanlan.zhihu.com/p/188671935#ref_3_0)[https://segmentfault.com/a/1190000016686335](https://segmentfault.com/a/1190000016686335)
4.  [^](https://zhuanlan.zhihu.com/p/188671935#ref_4_0)[https://blog.csdn.net/weixin\_38208741/article/details/78547248](https://blog.csdn.net/weixin_38208741/article/details/78547248)
5.  [^](https://zhuanlan.zhihu.com/p/188671935#ref_5_0)[https://zhuanlan.zhihu.com/p/87503403](https://zhuanlan.zhihu.com/p/87503403)