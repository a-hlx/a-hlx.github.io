## 19-卷积层

本讲文字介绍部分请参考沐神在线书籍~：https://zh-v2.d2l.ai/chapter_convolutional-neural-networks/why-conv.html

1.  _平移不变性_（translation invariance）：不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应，即为“平移不变性”。
    
2.  _局部性_（locality）：神经网络的前面几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系，这就是“局部性”原则。最终，可以聚合这些局部特征，以在整个图像级别进行预测。

![](https://cdn.jsdelivr.net/gh/HLIX1/pic/lm_ML/202210242124833.png)

#### 卷积

![](https://cdn.jsdelivr.net/gh/HLIX1/pic/lm_ML/202210242129819.png)


![](https://cdn.jsdelivr.net/gh/HLIX1/pic/lm_ML/202210242132465.png)


![](https://cdn.jsdelivr.net/gh/HLIX1/pic/lm_ML/202210242137856.png)

kernel size超参数

严格来说，卷积层是个错误的叫法，因为它所表达的运算其实是互相关运算（cross-correlation），而不是卷积运算。



#### 代码
```python
import torch
from torch import nn

# 二维互相关运算
def corr2d(X,K):    #X为输入，K为核矩阵
    h,w=K.shape    #h得到K的行数，w得到K的列数
    Y=torch.zeros((X.shape[0]-h+1,X.shape[1]-w+1))  #用0初始化输出矩阵Y
    for i in range(Y.shape[0]):   #卷积运算
        for j in range(Y.shape[1]):
          Y[i,j]=(X[i:i+h,j:j+w]*K).sum()
    return Y
```


```python
#样例点测试
X=torch.tensor([[0,1,2],[3,4,5],[6,7,8]])
K=torch.tensor([[0,1],[2,3]])
corr2d(X,K)
```




    >>> tensor([[19., 25.],
         	   [37., 43.]])


卷积层对输入和卷积核权重进行互相关运算，并在添加标量偏置之后产生输出。 所以，卷积层中的两个被训练的参数是卷积核权重和标量偏置。 就像我们之前随机初始化全连接层一样，在训练基于卷积层的模型时，我们也随机初始化卷积核权重。

```python
#实现二维卷积层
class Conv2d(nn.Module):
    def _init_(self,kernel_size):
        super()._init_()
        self.weight=nn.Parameter(torch.rand(kerner_size))
        self.bias=nn.Parameter(torch.zeros(1))
    def forward(self,x):
        return corr2d(x,self.weight)+self.bias 
```

如下是卷积层的一个简单应用：通过找到像素变化的位置，来检测图像中不同颜色的边缘。 首先，我们构造一个6×8像素的黑白图像。中间四列为黑色（0），其余像素为白色（1）。

```python
X=torch.ones((6,8))
X[:,2:6]=0
X
```


    >>> tensor([[1., 1., 0., 0., 0., 0., 1., 1.],
            [1., 1., 0., 0., 0., 0., 1., 1.],
            [1., 1., 0., 0., 0., 0., 1., 1.],
            [1., 1., 0., 0., 0., 0., 1., 1.],
            [1., 1., 0., 0., 0., 0., 1., 1.],
            [1., 1., 0., 0., 0., 0., 1., 1.]])


构造一个高度为1、宽度为2的卷积核`K`。当进行互相关运算时，如果水平相邻的两元素相同，则输出为零，否则输出为非零。

```python
K=torch.tensor([[-1,1]])  #这个K只能检测垂直边缘
Y=corr2d(X,K)
Y
```


输出`Y`中的1代表从白色到黑色的边缘，-1代表从黑色到白色的边缘，其他情况的输出为0。

    >>> tensor([[ 0., -1.,  0.,  0.,  0.,  1.,  0.],
                [ 0., -1.,  0.,  0.,  0.,  1.,  0.],
                [ 0., -1.,  0.,  0.,  0.,  1.,  0.],
                [ 0., -1.,  0.,  0.,  0.,  1.,  0.],
                [ 0., -1.,  0.,  0.,  0.,  1.,  0.],
                [ 0., -1.,  0.,  0.,  0.,  1.,  0.]])


将输入的二维图像转置，再进行如上的互相关运算。 其输出如下，之前检测到的垂直边缘消失了。 不出所料，这个卷积核`K`只可以检测垂直边缘，无法检测水平边缘。

```python
corr2d(X.t(),K)
```


    >>> tensor([[0., 0., 0., 0., 0.],
                [0., 0., 0., 0., 0.],
                [0., 0., 0., 0., 0.],
                [0., 0., 0., 0., 0.],
                [0., 0., 0., 0., 0.],
                [0., 0., 0., 0., 0.],
                [0., 0., 0., 0., 0.],
                [0., 0., 0., 0., 0.]])


##### 学习卷积核

现在让我们看看是否可以通过仅查看“输入-输出”对来学习由`X`生成`Y`的卷积核。 我们先构造一个卷积层，并将其卷积核初始化为随机张量。接下来，在每次迭代中，我们比较`Y`与卷积层输出的平方误差，然后计算梯度来更新卷积核。为了简单起见，我们在此使用内置的二维卷积层，并忽略偏置。

```python
conv2d = nn.Conv2d(1, 1, kernel_size=(1, 2), bias=False)

X = X.reshape((1, 1, 6, 8))
Y = Y.reshape((1, 1, 6, 7))

for i in range(10):
    Y_hat = conv2d(X)
    l = (Y_hat - Y)**2
    conv2d.zero_grad()
    l.sum().backward()
    conv2d.weight.data[:] -= 3e-2 * conv2d.weight.grad
    if (i + 1) % 2 == 0:
        print(f'batch {i+1}, loss {l.sum():.3f}')
```

    >>> batch 2, loss 3.852
        batch 4, loss 1.126
        batch 6, loss 0.386
        batch 8, loss 0.145
        batch 10, loss 0.057

在10次迭代之后，误差已经降到足够低。现在我们来看看我们所学的卷积核的权重张量。

```python
conv2d.weight.data.reshape((1, 2))
```

    >>> tensor([[-1.0173,  0.9685]])
