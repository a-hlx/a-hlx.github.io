
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="hlx的个人网站">
      
      
        <meta name="author" content="lix">
      
      
        <link rel="canonical" href="https://a-hlx.github.io/Foundation%20Course/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/DeepLearning_LHY21_Notes-master/11_Self-attention_P2/">
      
      
      
      <link rel="icon" href="../../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.4.2, mkdocs-material-9.0.6">
    
    
      
        <title>Self-attention_P2 - hlx个人网站</title>
      
    
    
      <link rel="stylesheet" href="../../../../assets/stylesheets/main.558e4712.min.css">
      
        
        <link rel="stylesheet" href="../../../../assets/stylesheets/palette.2505c338.min.css">
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../../mkdocs/css/no-footer.css">
    
      <link rel="stylesheet" href="../../../../mkdocs/css/unordered-list-symbols.css">
    
    <script>__md_scope=new URL("../../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="orange">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#self-attention_p2" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="页眉">
    <a href="../../../.." title="hlx个人网站" class="md-header__button md-logo" aria-label="hlx个人网站" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            hlx个人网站
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Self-attention_P2
            
          </span>
        </div>
      </div>
    </div>
    
      <form class="md-header__option" data-md-component="palette">
        
          
          <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="orange"  aria-label="切换至夜间模式"  type="radio" name="__palette" id="__palette_1">
          
            <label class="md-header__button md-icon" title="切换至夜间模式" for="__palette_2" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3 3.19.09m3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95 2.06.05m-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31Z"/></svg>
            </label>
          
        
          
          <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="orange"  aria-label="切换至日间模式"  type="radio" name="__palette" id="__palette_2">
          
            <label class="md-header__button md-icon" title="切换至日间模式" for="__palette_1" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5c-.84 0-1.65.15-2.39.42L12 2M3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29L3.34 7m.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14L3.36 17M20.65 7l-1.77 3.79a7.023 7.023 0 0 0-2.38-4.15l4.15.36m-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29L20.64 17M12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44L12 22Z"/></svg>
            </label>
          
        
      </form>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="查找">
        
        <button type="reset" class="md-search__icon md-icon" title="清空当前内容" aria-label="清空当前内容" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/a-hlx/a-hlx.github.io" title="前往仓库" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.2.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    a-hlx.github.io
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="标签" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../" class="md-tabs__link">
        Foundation Course
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../../cornerstone/" class="md-tabs__link">
        cornerstone
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../../LECTURE/" class="md-tabs__link">
        augment
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../../BLOG/" class="md-tabs__link">
        Blog
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../../REPRODUCE/" class="md-tabs__link">
        转载
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../../ABOUT/" class="md-tabs__link">
        关于
      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="导航栏" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../.." title="hlx个人网站" class="md-nav__button md-logo" aria-label="hlx个人网站" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    hlx个人网站
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/a-hlx/a-hlx.github.io" title="前往仓库" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.2.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    a-hlx.github.io
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
        
      
      <input class="md-nav__toggle md-toggle md-toggle--indeterminate" data-md-toggle="__nav_1" type="checkbox" id="__nav_1" >
      
      
        
          
            
          
        
          
        
          
        
          
        
      
      
        
        
        <div class="md-nav__link md-nav__link--index ">
          <a href="../../../">Foundation Course</a>
          
            <label for="__nav_1">
              <span class="md-nav__icon md-icon"></span>
            </label>
          
        </div>
      
      <nav class="md-nav" aria-label="Foundation Course" data-md-level="1">
        <label class="md-nav__title" for="__nav_1">
          <span class="md-nav__icon md-icon"></span>
          Foundation Course
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
        
      
      <input class="md-nav__toggle md-toggle md-toggle--indeterminate" data-md-toggle="__nav_1_2" type="checkbox" id="__nav_1_2" >
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_1_2" tabindex="0" aria-expanded="false">
          python
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="python" data-md-level="2">
        <label class="md-nav__title" for="__nav_1_2">
          <span class="md-nav__icon md-icon"></span>
          python
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
        
      
      <input class="md-nav__toggle md-toggle md-toggle--indeterminate" data-md-toggle="__nav_1_2_1" type="checkbox" id="__nav_1_2_1" >
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_1_2_1" tabindex="0" aria-expanded="false">
          python基础
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="python基础" data-md-level="3">
        <label class="md-nav__title" for="__nav_1_2_1">
          <span class="md-nav__icon md-icon"></span>
          python基础
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Python/python%E5%9F%BA%E7%A1%80/Day01%20python%E5%9F%BA%E7%A1%80/" class="md-nav__link">
        01 绪论
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Python/python%E5%9F%BA%E7%A1%80/Day02%20if%E5%88%A4%E6%96%AD%E4%B8%8E%E5%BE%AA%E7%8E%AF/" class="md-nav__link">
        02 if判断与循环
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Python/python%E5%9F%BA%E7%A1%80/Day03%20%E5%AE%B9%E5%99%A8/" class="md-nav__link">
        03 容器
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Python/python%E5%9F%BA%E7%A1%80/Day04%20%E5%AD%97%E5%85%B8%E4%B8%8E%E5%87%BD%E6%95%B0/" class="md-nav__link">
        04 字典与函数
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Python/python%E5%9F%BA%E7%A1%80/Day05%20%E5%87%BD%E6%95%B0/" class="md-nav__link">
        05 函数
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Python/python%E5%9F%BA%E7%A1%80/Day06%20%E5%87%BD%E6%95%B0%E4%B8%8E%E6%96%87%E4%BB%B6/" class="md-nav__link">
        06 函数与文件
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Python/python%E5%9F%BA%E7%A1%80/Day07%20%E6%96%87%E4%BB%B6/" class="md-nav__link">
        07 文件
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Python/python%E5%9F%BA%E7%A1%80/Day08%20%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1_%E5%B0%81%E8%A3%85/" class="md-nav__link">
        08 面向对象_封装
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Python/python%E5%9F%BA%E7%A1%80/Day09%20%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/" class="md-nav__link">
        09 面向对象
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Python/python%E5%9F%BA%E7%A1%80/Day10%20%E5%BC%82%E5%B8%B8%E4%B8%8E%E6%A8%A1%E5%9D%97/" class="md-nav__link">
        10 异常与模块
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
        
      
      <input class="md-nav__toggle md-toggle md-toggle--indeterminate" data-md-toggle="__nav_1_3" type="checkbox" id="__nav_1_3" >
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_1_3" tabindex="0" aria-expanded="false">
          MATLAB
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="MATLAB" data-md-level="2">
        <label class="md-nav__title" for="__nav_1_3">
          <span class="md-nav__icon md-icon"></span>
          MATLAB
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../MATLAB/02Array_Operation/" class="md-nav__link">
        02Array_Operation
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../MATLAB/03Structured_Programming_%26_Function/" class="md-nav__link">
        03Structured_Programming_&_Function
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../MATLAB/04Data_Structure_%26_File_Access/" class="md-nav__link">
        04Data_Structure_&_File_Access
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../MATLAB/05Basic_Plotting/" class="md-nav__link">
        05Basic_Plotting
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../MATLAB/06Advanced_Plotting/" class="md-nav__link">
        06Advanced_Plotting
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../MATLAB/08Image_Processing%20I/" class="md-nav__link">
        08Image_Processing I
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../MATLAB/09Image_Processing%20II/" class="md-nav__link">
        09Image_Processing II
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../MATLAB/10Integration_%26_Differentiation/" class="md-nav__link">
        10Integration_&_Differentiation
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../MATLAB/11Root_Finding/" class="md-nav__link">
        11Root_Finding
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../MATLAB/12Linear_Equations/" class="md-nav__link">
        12Linear_Equations
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../MATLAB/13Statistics_%26_Data_Analysis/" class="md-nav__link">
        13Statistics_&_Data_Analysis
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../MATLAB/14Curve_Fitting_%26_Interpolation/" class="md-nav__link">
        14Curve_Fitting_&_Interpolation
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../MATLAB/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%E6%95%B0%E5%80%BC%E8%A7%A3/" class="md-nav__link">
        微分方程数值解
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
        
      
      <input class="md-nav__toggle md-toggle md-toggle--indeterminate" data-md-toggle="__nav_1_4" type="checkbox" id="__nav_1_4" >
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_1_4" tabindex="0" aria-expanded="false">
          MYSQL
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="MYSQL" data-md-level="2">
        <label class="md-nav__title" for="__nav_1_4">
          <span class="md-nav__icon md-icon"></span>
          MYSQL
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../MySQL/%E7%9B%AE%E5%BD%95/" class="md-nav__link">
        00.目录
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../MySQL/01.mysql%E5%AE%89%E8%A3%85/" class="md-nav__link">
        01.mysql安装
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../MySQL/02.%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/" class="md-nav__link">
        02.数据类型
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../MySQL/03.%E5%BA%93%E5%92%8C%E6%95%B0%E6%8D%AE%E8%A1%A8/" class="md-nav__link">
        03.库和数据表
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../MySQL/04.%E8%A1%A8%E7%9A%84%E5%A2%9E%E5%88%A0%E6%94%B9%E6%9F%A5/" class="md-nav__link">
        04.表的增删改查
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../MySQL/05.%E8%A1%A8%E7%9A%84%E7%BB%BC%E5%90%88%E6%9F%A5%E8%AF%A2/" class="md-nav__link">
        05.表的综合查询
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../MySQL/06.%E7%B4%A2%E5%BC%95/" class="md-nav__link">
        06.索引
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../MySQL/07.%E4%BA%8B%E5%8A%A1/" class="md-nav__link">
        07.事务
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../MySQL/08.mysql%E7%94%A8%E6%88%B7%E4%B8%8E%E6%9D%83%E9%99%90%E7%AE%A1%E7%90%86/" class="md-nav__link">
        08.mysql用户与权限管理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../MySQL/09.mysql%E4%BC%98%E5%8C%96/" class="md-nav__link">
        09.mysql优化
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../MySQL/10.mysql%E4%BC%98%E5%8C%96%E5%88%86%E6%9E%90/" class="md-nav__link">
        10.mysql优化分析
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../MySQL/11.%E8%A1%A8%E5%88%86%E5%8C%BA/" class="md-nav__link">
        11.表分区
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../MySQL/12.mysql%E9%9B%86%E7%BE%A4/" class="md-nav__link">
        12.mysql集群
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../MySQL/13.%E5%BC%80%E5%8F%91%E4%B8%AD%E6%AF%94%E8%BE%83%E5%B0%91%E7%94%A8%E7%9A%84%E5%8A%9F%E8%83%BD/" class="md-nav__link">
        13.开发中比较少用的功能
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../MySQL/14.%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AF%BC%E5%85%A5%E5%AF%BC%E5%87%BA/" class="md-nav__link">
        14.数据库导入导出
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../MySQL/15.%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%92%8C%E6%9C%8D%E5%8A%A1%E7%AB%AF%E5%AE%89%E5%85%A8%E4%BC%A0%E8%BE%93%E6%95%B0%E6%8D%AE/" class="md-nav__link">
        15.客户端和服务端安全传输数据
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../MySQL/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/" class="md-nav__link">
        常见问题
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
        
      
      <input class="md-nav__toggle md-toggle md-toggle--indeterminate" data-md-toggle="__nav_2" type="checkbox" id="__nav_2" >
      
      
        
          
            
          
        
          
        
          
        
      
      
        
        
        <div class="md-nav__link md-nav__link--index ">
          <a href="../../../../cornerstone/">cornerstone</a>
          
            <label for="__nav_2">
              <span class="md-nav__icon md-icon"></span>
            </label>
          
        </div>
      
      <nav class="md-nav" aria-label="cornerstone" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          cornerstone
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
        
      
      <input class="md-nav__toggle md-toggle md-toggle--indeterminate" data-md-toggle="__nav_2_2" type="checkbox" id="__nav_2_2" >
      
      
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_2_2" tabindex="0" aria-expanded="false">
          Git
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Git" data-md-level="2">
        <label class="md-nav__title" for="__nav_2_2">
          <span class="md-nav__icon md-icon"></span>
          Git
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../cornerstone/Git/10%20%E4%B8%AA%E8%8A%82%E7%9C%81%E6%97%B6%E9%97%B4%E5%92%8C%E6%94%B9%E5%96%84%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%9A%84%20Git%20%E6%8A%80%E5%B7%A7/" class="md-nav__link">
        Git技巧
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../cornerstone/Git/Git%28%E5%88%86%E5%B8%83%E5%BC%8F%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6%E5%B7%A5%E5%85%B7%29/" class="md-nav__link">
        Git基础
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../cornerstone/Git/git%E9%80%9F%E6%9F%A5/" class="md-nav__link">
        Git速查
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
        
      
      <input class="md-nav__toggle md-toggle md-toggle--indeterminate" data-md-toggle="__nav_2_3" type="checkbox" id="__nav_2_3" >
      
      
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_2_3" tabindex="0" aria-expanded="false">
          Markdown
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Markdown" data-md-level="2">
        <label class="md-nav__title" for="__nav_2_3">
          <span class="md-nav__icon md-icon"></span>
          Markdown
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../cornerstone/Markdown/markdown/" class="md-nav__link">
        Markdown讲座
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../cornerstone/Markdown/vscode-extension/" class="md-nav__link">
        VSCode及插件安装
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../cornerstone/Markdown/grammars/" class="md-nav__link">
        Markdown语法初学习
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../cornerstone/Markdown/ExMark-spec-zh.txt" class="md-nav__link">
        TODO ExMark规范
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../cornerstone/Markdown/MarkDown%E8%B6%85%E7%BA%A7%E6%95%99%E7%A8%8B%20Obsidian%E7%89%88%202022.1.12/" class="md-nav__link">
        MarkDown教程Ob版
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
        
      
      <input class="md-nav__toggle md-toggle md-toggle--indeterminate" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" >
      
      
        
          
            
          
        
          
        
      
      
        
        
        <div class="md-nav__link md-nav__link--index ">
          <a href="../../../../LECTURE/">augment</a>
          
            <label for="__nav_3">
              <span class="md-nav__icon md-icon"></span>
            </label>
          
        </div>
      
      <nav class="md-nav" aria-label="augment" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          augment
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
        
      
      <input class="md-nav__toggle md-toggle md-toggle--indeterminate" data-md-toggle="__nav_3_2" type="checkbox" id="__nav_3_2" >
      
      
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_2" tabindex="0" aria-expanded="false">
          Linux
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Linux" data-md-level="2">
        <label class="md-nav__title" for="__nav_3_2">
          <span class="md-nav__icon md-icon"></span>
          Linux
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../LECTURE/Linux/linux/" class="md-nav__link">
        Linux讲座
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../LECTURE/Linux/commands-concise/" class="md-nav__link">
        命令行入门精简版
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
        
      
      <input class="md-nav__toggle md-toggle md-toggle--indeterminate" data-md-toggle="__nav_4" type="checkbox" id="__nav_4" >
      
      
        
          
            
          
        
          
        
          
        
          
        
      
      
        
        
        <div class="md-nav__link md-nav__link--index ">
          <a href="../../../../BLOG/">Blog</a>
          
            <label for="__nav_4">
              <span class="md-nav__icon md-icon"></span>
            </label>
          
        </div>
      
      <nav class="md-nav" aria-label="Blog" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          Blog
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
        
      
      <input class="md-nav__toggle md-toggle md-toggle--indeterminate" data-md-toggle="__nav_4_2" type="checkbox" id="__nav_4_2" >
      
      
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_4_2" tabindex="0" aria-expanded="false">
          software
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="software" data-md-level="2">
        <label class="md-nav__title" for="__nav_4_2">
          <span class="md-nav__icon md-icon"></span>
          software
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../BLOG/software/%E5%85%8D%E8%B4%B9%E6%BF%80%E6%B4%BBTypora/" class="md-nav__link">
        免费激活Typora
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../BLOG/software/Obsidian%E4%BB%8B%E7%BB%8D/" class="md-nav__link">
        Obsidian介绍
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
        
      
      <input class="md-nav__toggle md-toggle md-toggle--indeterminate" data-md-toggle="__nav_4_3" type="checkbox" id="__nav_4_3" >
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_4_3" tabindex="0" aria-expanded="false">
          Fun learning
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Fun learning" data-md-level="2">
        <label class="md-nav__title" for="__nav_4_3">
          <span class="md-nav__icon md-icon"></span>
          Fun learning
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../BLOG/Fun%20learning/ChatGPT%E4%B8%AD%E6%96%87%E8%B0%83%E6%95%99%E6%8C%87%E5%8D%97/" class="md-nav__link">
        ChatGPT中文调教指南
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
        
      
      <input class="md-nav__toggle md-toggle md-toggle--indeterminate" data-md-toggle="__nav_4_4" type="checkbox" id="__nav_4_4" >
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_4_4" tabindex="0" aria-expanded="false">
          速查表
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="速查表" data-md-level="2">
        <label class="md-nav__title" for="__nav_4_4">
          <span class="md-nav__icon md-icon"></span>
          速查表
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../BLOG/%E9%80%9F%E6%9F%A5%E8%A1%A8/bash/" class="md-nav__link">
        bash
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../BLOG/%E9%80%9F%E6%9F%A5%E8%A1%A8/c/" class="md-nav__link">
        c
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../BLOG/%E9%80%9F%E6%9F%A5%E8%A1%A8/cmd/" class="md-nav__link">
        cmd
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../BLOG/%E9%80%9F%E6%9F%A5%E8%A1%A8/docker/" class="md-nav__link">
        docker
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../BLOG/%E9%80%9F%E6%9F%A5%E8%A1%A8/emoji/" class="md-nav__link">
        emoji
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../BLOG/%E9%80%9F%E6%9F%A5%E8%A1%A8/git/" class="md-nav__link">
        git
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../BLOG/%E9%80%9F%E6%9F%A5%E8%A1%A8/github-actions/" class="md-nav__link">
        github-actions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../BLOG/%E9%80%9F%E6%9F%A5%E8%A1%A8/github/" class="md-nav__link">
        github
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../BLOG/%E9%80%9F%E6%9F%A5%E8%A1%A8/gitlab-ci/" class="md-nav__link">
        gitlab-ci
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../BLOG/%E9%80%9F%E6%9F%A5%E8%A1%A8/gitlab/" class="md-nav__link">
        gitlab
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../BLOG/%E9%80%9F%E6%9F%A5%E8%A1%A8/gmail/" class="md-nav__link">
        gmail
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../BLOG/%E9%80%9F%E6%9F%A5%E8%A1%A8/html-char/" class="md-nav__link">
        html-char
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../BLOG/%E9%80%9F%E6%9F%A5%E8%A1%A8/html/" class="md-nav__link">
        html
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../BLOG/%E9%80%9F%E6%9F%A5%E8%A1%A8/java/" class="md-nav__link">
        java
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../BLOG/%E9%80%9F%E6%9F%A5%E8%A1%A8/javascript/" class="md-nav__link">
        javascript
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../BLOG/%E9%80%9F%E6%9F%A5%E8%A1%A8/latex/" class="md-nav__link">
        latex
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../BLOG/%E9%80%9F%E6%9F%A5%E8%A1%A8/markdown/" class="md-nav__link">
        markdown
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../BLOG/%E9%80%9F%E6%9F%A5%E8%A1%A8/matlab/" class="md-nav__link">
        matlab
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../BLOG/%E9%80%9F%E6%9F%A5%E8%A1%A8/mysql/" class="md-nav__link">
        mysql
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../BLOG/%E9%80%9F%E6%9F%A5%E8%A1%A8/pycharm/" class="md-nav__link">
        pycharm
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../BLOG/%E9%80%9F%E6%9F%A5%E8%A1%A8/python/" class="md-nav__link">
        python
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../BLOG/%E9%80%9F%E6%9F%A5%E8%A1%A8/pytorch/" class="md-nav__link">
        pytorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../BLOG/%E9%80%9F%E6%9F%A5%E8%A1%A8/ssh/" class="md-nav__link">
        ssh
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../BLOG/%E9%80%9F%E6%9F%A5%E8%A1%A8/vscode/" class="md-nav__link">
        vscode
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
        
      
      <input class="md-nav__toggle md-toggle md-toggle--indeterminate" data-md-toggle="__nav_5" type="checkbox" id="__nav_5" >
      
      
        
          
            
          
        
          
        
          
        
          
        
      
      
        
        
        <div class="md-nav__link md-nav__link--index ">
          <a href="../../../../REPRODUCE/">转载</a>
          
            <label for="__nav_5">
              <span class="md-nav__icon md-icon"></span>
            </label>
          
        </div>
      
      <nav class="md-nav" aria-label="转载" data-md-level="1">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          转载
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../REPRODUCE/how-to-ask-questions-the-smart-way/" class="md-nav__link">
        提问的智慧
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../REPRODUCE/tim-cook-at-duke/" class="md-nav__link">
        Tim Cook's Commencement Address at Duke University, 2018
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../REPRODUCE/my-best-teacher/" class="md-nav__link">
        我最好的老师
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
        
      
      <input class="md-nav__toggle md-toggle md-toggle--indeterminate" data-md-toggle="__nav_6" type="checkbox" id="__nav_6" >
      
      
        
          
            
          
        
          
        
          
        
          
        
      
      
        
        
        <div class="md-nav__link md-nav__link--index ">
          <a href="../../../../ABOUT/">关于</a>
          
            <label for="__nav_6">
              <span class="md-nav__icon md-icon"></span>
            </label>
          
        </div>
      
      <nav class="md-nav" aria-label="关于" data-md-level="1">
        <label class="md-nav__title" for="__nav_6">
          <span class="md-nav__icon md-icon"></span>
          关于
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../ABOUT/help/" class="md-nav__link">
        文章编写帮助
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../ABOUT/build/" class="md-nav__link">
        网站构建
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../ABOUT/test/" class="md-nav__link">
        测试文档
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    矩阵的角度
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#multi-head-self-attention" class="md-nav__link">
    Multi-head Self-attention
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#positional-encoding" class="md-nav__link">
    Positional Encoding
  </a>
  
    <nav class="md-nav" aria-label="Positional Encoding">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#no-position-information-in-self-attention" class="md-nav__link">
    No position information in self-attention
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#each-positon-has-a-unique-positional-vector-ei" class="md-nav__link">
    Each positon has a unique positional vector \(e^i\)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hand-crafted-or-learned-from-data" class="md-nav__link">
    Hand-crafted or Learned from data
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#applications" class="md-nav__link">
    Applications …
  </a>
  
    <nav class="md-nav" aria-label="Applications …">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#self-attention-for-speech" class="md-nav__link">
    Self-attention for Speech
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#self-attention-for-image" class="md-nav__link">
    Self-attention for Image
  </a>
  
    <nav class="md-nav" aria-label="Self-attention for Image">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#self-attention-vs-cnn" class="md-nav__link">
    Self-attention v.s. CNN
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#self-attention-vs-rnn" class="md-nav__link">
    Self-attention v.s. RNN
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#self-attention-for-graph" class="md-nav__link">
    Self-attention for Graph
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#more" class="md-nav__link">
    More
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  
  



  
  



<h1 id="self-attention_p2">Self-attention_P2<a class="headerlink" href="#self-attention_p2" title="Permanent link">&para;</a></h1>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210409092350116.png" alt="image-20210409092350116" style="zoom:50%;" /></p>
<p>从这一排 vector 得到 <span class="arithmatex">\(b^1\)</span>,跟从这一排 vector 得到 <span class="arithmatex">\(b^2\)</span>,它的操作是一模一样的.要强调一点是,这边的 <span class="arithmatex">\(b^1\)</span> 到 <span class="arithmatex">\(b^4\)</span>,它们并<strong>不需要依序產生</strong>,它们是一次同时被计算出来的</p>
<p>怎麼计算这个 <span class="arithmatex">\(b^2\)</span>？我们现在的主角,就变成 <span class="arithmatex">\(a^2\)</span></p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210409093744204.png" alt="image-20210409093744204" style="zoom:50%;" /></p>
<ul>
<li>
<p>把 <span class="arithmatex">\(a^2\)</span> 乘上一个 matrix,变成 <span class="arithmatex">\(q^2\)</span></p>
</li>
<li>
<p>然后接下来根据 <span class="arithmatex">\(q^2\)</span>,去对<span class="arithmatex">\(a^1\)</span>到 <span class="arithmatex">\(a^4\)</span> 这四个位置,都去计算 attention 的 score</p>
<ul>
<li>把 <span class="arithmatex">\(q^2\)</span> 跟 <span class="arithmatex">\(k^1\)</span> 做个这个 dot product</li>
<li>把 <span class="arithmatex">\(q^2\)</span> 跟 <span class="arithmatex">\(k^2\)</span> 也做个 dot product</li>
<li>把 <span class="arithmatex">\(q^2\)</span> 跟 <span class="arithmatex">\(k^3\)</span> 也做 dot product</li>
<li>把 <span class="arithmatex">\(q^2\)</span> 跟 <span class="arithmatex">\(k^4\)</span> 也做 dot product,得到四个分数</li>
</ul>
</li>
<li>
<p>得到这四个分数以后,可能还会做一个 normalization,比如说 softmax,然后得到最后的 attention 的 score,<span class="arithmatex">\(α'_{2,1} \space α'_{2,2}  \space α'_{2,3}  \space α'_{2,4}\)</span>那我们这边用 <span class="arithmatex">\(α'\)</span>表示经过 normalization 以后的attention score</p>
</li>
<li>
<p>接下来拿这四个数值,分别乘上 <span class="arithmatex">\(v^1  \space  v^2 \space  v^3 \space  v^4\)</span></p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210409094148657.png" alt="image-20210409094148657" style="zoom:50%;" /></p>
<ul>
<li>把 <span class="arithmatex">\(α'_{2,1}\)</span>乘上 <span class="arithmatex">\(v^1\)</span></li>
<li>把 <span class="arithmatex">\(α'_{2,2}\)</span> 乘上 <span class="arithmatex">\(v^2\)</span></li>
<li>把 <span class="arithmatex">\(α'_{2,3}\)</span> 乘上 <span class="arithmatex">\(v^3\)</span></li>
<li>把 <span class="arithmatex">\(α'_{2,4}\)</span> 乘上 <span class="arithmatex">\(v^4\)</span>,然后全部加起来就是 $ b^2$</li>
</ul>
<div class="arithmatex">\[
b^2=\sum_iα'_{2,i}v^i
\]</div>
</li>
</ul>
<p>同理就可以,由 <span class="arithmatex">\(a^3\)</span> 乘一个 transform 得到 <span class="arithmatex">\(q^3\)</span>,然后就计算 <span class="arithmatex">\(b^3\)</span>,从 <span class="arithmatex">\(a^4\)</span> 乘一个 transform 得到 <span class="arithmatex">\(q^4\)</span>,就计算 <span class="arithmatex">\(b^4\)</span>,以上说的是  Self-attention 它运作的过程</p>
<h2 id="_1">矩阵的角度<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h2>
<p>接下来我们从矩阵乘法的角度,再重新讲一次我们刚才讲的,Self-attention 是怎麼运作的</p>
<p>我们现在已经知道每一个 a 都產生 q k v</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210409100334528.png" alt="image-20210409100334528" style="zoom:50%;" /></p>
<p>如果要用矩阵运算表示这个操作的话,是什麼样子呢</p>
<p>我们每一个 a,都乘上一个矩阵,我们这边用 <span class="arithmatex">\(W^q\)</span> 来表示它,得到 <span class="arithmatex">\(q^i\)</span>,每一个 a 都要乘上 <span class="arithmatex">\(W^q\)</span>,得到<span class="arithmatex">\(q^i\)</span>,<strong>这些不同的 a 你可以把它合起来,当作一个矩阵来看待</strong></p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210409100755718.png" alt="image-20210409100755718" style="zoom:67%;" /></p>
<p>一样$a^2\space a^3\space a^4 <span class="arithmatex">\(也都乘上 <span class="arithmatex">\(W^q\)</span> 得到\)</span>q^2 q^3 $跟 <span class="arithmatex">\(q^4\)</span>,那你可以<strong>把 a1 到 a4 拼起来</strong>,看作是一个矩阵,这个矩阵我们用 I 来表示，这个矩阵的四个 column 就是 <span class="arithmatex">\(a^1\)</span> 到 <span class="arithmatex">\(a^4\)</span></p>
<p><span class="arithmatex">\(I\)</span> 乘上 <span class="arithmatex">\(W^q\)</span> 就得到另外一个矩阵,我们用 <span class="arithmatex">\(Q\)</span> 来表示它,这个 <span class="arithmatex">\(Q\)</span> 就是把 <span class="arithmatex">\(q^1\)</span> 到 <span class="arithmatex">\(q^4\)</span> 这四个 vector 拼起来,就是 <span class="arithmatex">\(Q\)</span> 的四个 column</p>
<p>所以我们从 <span class="arithmatex">\(a^1\)</span> 到 <span class="arithmatex">\(a^4\)</span>,得到  <span class="arithmatex">\(q^1\)</span> 到 <span class="arithmatex">\(q^4\)</span>这个操作,其实就是<strong>把 I 这个矩阵,乘上另外一个矩阵 <span class="arithmatex">\(W^q\)</span>，得到矩阵<span class="arithmatex">\(Q\)</span></strong>。<span class="arithmatex">\(I\)</span> 这个矩阵它裡面的 column就是我们 Self-attention 的 input是 <span class="arithmatex">\(a^1\)</span> 到 <span class="arithmatex">\(a^4\)</span>；<strong><span class="arithmatex">\(W^q\)</span>其实是 network 的参数,它是等一下会被learn出来的</strong> ；<span class="arithmatex">\(Q\)</span> 的四个 column,就是  <span class="arithmatex">\(q^1\)</span> 到 <span class="arithmatex">\(q^4\)</span></p>
<p>接下来產生 k 跟 v 的操作跟 q 是一模一样的</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210409101331347.png" alt="image-20210409101331347" style="zoom:67%;" /></p>
<p>所以每一个 a 得到 q k v ,其实就是把输入的这个,vector sequence 乘上三个不同的矩阵,你就得到了 q,得到了 k,跟得到了 v</p>
<p>下一步是,每一个 q 都会去跟每一个 k,去计算这个 inner product,去<strong>得到这个 attention 的分数</strong></p>
<p>那得到 attention 分数这一件事情,如果从矩阵操作的角度来看,它在做什麼样的事情呢</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210409102703364.png" alt="image-20210409102703364" style="zoom: 50%;" /></p>
<p>你就是把 <span class="arithmatex">\(q^1\)</span> 跟 <span class="arithmatex">\(k^1\)</span> 做 inner product,得到 <span class="arithmatex">\(α_{1,1}\)</span>,所以 <span class="arithmatex">\(α_{1,1}\)</span>就是 <span class="arithmatex">\(q^1\)</span> 跟<span class="arithmatex">\(k^1\)</span> 的 inner product,那这边我就把这个,<span class="arithmatex">\(k^1\)</span>它背后的这个向量,把它画成比较宽一点代表说它是 transpose</p>
<p>同理 <span class="arithmatex">\(α_{1,2}\)</span> 就是 <span class="arithmatex">\(q^1\)</span> 跟 <span class="arithmatex">\(k^2\)</span>,做 inner product, <span class="arithmatex">\(α_{1,3}\)</span>  就是 <span class="arithmatex">\(q^1\)</span> 跟 <span class="arithmatex">\(k^3\)</span> 做 inner product,这个  <span class="arithmatex">\(α_{1,4}\)</span>  就是 <span class="arithmatex">\(q^1\)</span> 跟 <span class="arithmatex">\(k^4\)</span> 做 inner product</p>
<p>那这个四个步骤的操作,你其实可以把它拼起来,看作是<strong>矩阵跟向量相乘</strong></p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210409102832459.png" alt="image-20210409102832459" style="zoom:67%;" /></p>
<p>这四个动作,你可以看作是我们<strong>把 <span class="arithmatex">\(k^1\)</span> 到 <span class="arithmatex">\(k^4\)</span> 拼起来,当作是一个矩阵的四个 row</strong></p>
<p>那我们刚才讲过说,我们不只是 <span class="arithmatex">\(q^1\)</span>,要对<span class="arithmatex">\(k^1\)</span> 到 <span class="arithmatex">\(k^4\)</span> 计算 attention,<span class="arithmatex">\(q^2,q^3,q^4\)</span>也要对 <span class="arithmatex">\(k^1\)</span> 到 <span class="arithmatex">\(k^4\)</span> 计算 attention,操作其实都是一模一样的</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210409103622596.png" alt="image-20210409103622596" style="zoom:67%;" /></p>
<p>所以这些 <strong>attention 的分数可以看作是两个矩阵的相乘</strong>,一个矩阵它的 row,就是 <span class="arithmatex">\(k^1\)</span> 到 <span class="arithmatex">\(k^4\)</span>,另外一个矩阵它的 column </p>
<p>我们会在 attention 的分数,<strong>做一下 normalization</strong>,比如说你会做 softmax,你会对这边的每一个 column,每一个 column 做 softmax,让每一个 column 裡面的值相加是 1</p>
<p>之前有讲过说 其实这边做 <strong>softmax不是唯一的选项</strong>,你完全可以选择其他的操作,比如说 ReLU 之类的,那其实得到的结果也不会比较差,通过了 softmax 以后,它得到的值有点不一样了,所以我们用 <span class="arithmatex">\(A'\)</span>,来表示通过 softmax 以后的结果</p>
<p>我们已经计算出 $A' $</p>
<p>那我们把这个<span class="arithmatex">\(v^1\)</span> 到 <span class="arithmatex">\(v^4\)</span>乘上这边的 α 以后,就可以得到 b</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210409105513608.png" alt="image-20210409105513608" style="zoom:67%;" /></p>
<p>你就把<span class="arithmatex">\(v^1\)</span> 到 <span class="arithmatex">\(v^4\)</span> 拼起来,你<strong>把 <span class="arithmatex">\(v^1\)</span> 到 <span class="arithmatex">\(v^4\)</span>当成是V 这个矩阵的四个 column</strong>,把它拼起来,然后接下来你把 v 乘上,<span class="arithmatex">\(A'\)</span> 的第一个 column 以后,你得到的结果就是 <span class="arithmatex">\(b^1\)</span></p>
<p>如果你熟悉线性代数的话,你知道说把这个 <span class="arithmatex">\(A'\)</span> 乘上 V,就是把 <span class="arithmatex">\(A'\)</span>的第一个 column,乘上 V 这一个矩阵,你会得到你 output 矩阵的第一个 column</p>
<p>而把 A 的第一个 column乘上 V 这个矩阵做的事情,其实就是把 V 这个矩阵裡面的每一个 column,<strong>根据第 <span class="arithmatex">\(A'\)</span> 这个矩阵裡面的每一个 column 裡面每一个 element,做 weighted sum</strong>,那就得到 <span class="arithmatex">\(b^1\)</span></p>
<p>那就是这边的操作,把 <span class="arithmatex">\(v^1\)</span> 到 <span class="arithmatex">\(v^4\)</span> 乘上 weight,全部加起来得到 <span class="arithmatex">\(b^1\)</span>,</p>
<p>如果你是用矩阵操作的角度来看它,就是把$ A'$ 的第一个 column 乘上 V,就得到 <span class="arithmatex">\(b^1\)</span>,然后接下来就是以此类推</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210409105935046.png" alt="image-20210409105935046" style="zoom: 67%;" /></p>
<p>就是以此类推,把 <span class="arithmatex">\(A'\)</span> 的第二个 column 乘上 V,就得到 <span class="arithmatex">\(b^2\)</span>,<span class="arithmatex">\(A'\)</span> 的第三个 column 乘上 V 就得到 <span class="arithmatex">\(b^3\)</span>,<span class="arithmatex">\(A'\)</span> 的最后一个 column 乘上 V,就得到 <span class="arithmatex">\(b^4\)</span></p>
<p>所以我们等於就是把 <span class="arithmatex">\(A'\)</span> 这个矩阵,乘上 V 这个矩阵,得到 O 这个矩阵,O 这个矩阵裡面的每一个 column,就是 Self-attention 的输出,也就是 <span class="arithmatex">\(b^1\)</span> 到 <span class="arithmatex">\(b^4\)</span>,</p>
<p>所以其实整个 Self-attention,我们在讲操作的时候,我们在最开始的时候 跟你讲的时候我们讲说,我们先產生了 q k v,然后再根据这个 q 去找出相关的位置,然后再对 v 做 weighted sum,其实这一串操作,<strong>就是一连串矩阵的乘法而已</strong></p>
<p>我们再复习一下我们刚才看到的矩阵乘法</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210409110638357.png" alt="image-20210409110638357" style="zoom:50%;" /></p>
<ul>
<li>
<p>I 是 Self-attention 的 input,Self-attention 的 input 是一排的vector,这排 vector 拼起来当作矩阵的 column,就是 I</p>
</li>
<li>
<p>这个 input 分别乘上三个矩阵,<span class="arithmatex">\(W^q\)</span> <span class="arithmatex">\(W^k\)</span> 跟$ W^v$,得到 Q K V </p>
</li>
<li>这三个矩阵,接下来 Q 乘上 K 的 transpose,得到 A 这个矩阵,A 的矩阵你可能会做一些处理,得到 <span class="arithmatex">\(A'\)</span>,那有时候我们会把这个 <span class="arithmatex">\(A'\)</span>,叫做 ==Attention Matrix==，<strong>生成Q矩阵就是为了得到Attention的score</strong></li>
<li>然后接下来你把 <span class="arithmatex">\(A'\)</span> 再乘上 V,就得到 O,O 就是 Self-attention 这个 layer 的输出,<strong>生成V是为了计算最后的b，也就是矩阵O</strong></li>
</ul>
<p>所以 Self-attention 输入是 I,输出是 O,那你会发现说虽然是叫 attention,但是<strong>其实 Self-attention layer 裡面,唯一需要学的参数,就只有 <span class="arithmatex">\(W^q\)</span> <span class="arithmatex">\(W^k\)</span> 跟$ W^v$ 而已,只有<span class="arithmatex">\(W^q\)</span> <span class="arithmatex">\(W^k\)</span> 跟$ W^v$是未知的</strong>,是需要透过我们的训练资料把它找出来的</p>
<p>但是其他的操作都没有未知的参数,都是我们人為设定好的,都不需要透过 training data 找出来,那这整个就是 Self-attention 的操作,从 I 到 O 就是做了 Self-attention</p>
<h2 id="multi-head-self-attention">Multi-head Self-attention<a class="headerlink" href="#multi-head-self-attention" title="Permanent link">&para;</a></h2>
<p>Self-attention 有一个进阶的版本,叫做 ==Multi-head Self-attention==, Multi-head Self-attention,其实今天的使用是非常地广泛的</p>
<p>在作业 4 裡面,助教原来的 code 4 有,Multi-head Self-attention,它的 head 的数目是设成 2,那刚才助教有给你提示说,把 head 的数目改少一点 改成 1,其实就可以过medium baseline</p>
<p>但并不代表所有的任务,都适合用比较少的 head,有一些任务,比如说翻译,比如说语音辨识,其实用比较多的 head,你反而可以得到比较好的结果</p>
<p>至於<strong>需要用多少的 head,这个又是另外一个 hyperparameter</strong>,也是你需要调的</p>
<p>那為什麼我们会需要比较多的 head 呢,你可以想成说相关这件事情</p>
<p>我们在做这个 Self-attention 的时候,我们就是用 q 去找相关的 k,但是<strong>==相关==这件事情有很多种不同的形式</strong>,有很多种不同的定义,所以也许我们不能只有一个 q,我们应该要有多个 q,<strong>不同的 q 负责不同种类的相关性</strong></p>
<p>所以假设你要做 Multi-head Self-attention 的话,你会怎麼操作呢?</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210412193656512.png" alt="image-20210412193656512" style="zoom:50%;" /></p>
<ul>
<li>先把 a 乘上一个矩阵得到 q</li>
<li>再把 q 乘上另外两个矩阵,分别得到 <span class="arithmatex">\(q^1\)</span> 跟 <span class="arithmatex">\(q^2\)</span>,那这边还有 这边是用两个上标,i 代表的是位置,然后这个 1 跟 2 代表是,这个位置的第几个 q,所以这边有 <span class="arithmatex">\(q^{i,1}\)</span> 跟 <span class="arithmatex">\(q^{i,2}\)</span>,代表说我们有两个 head</li>
</ul>
<p>我们认為这个问题,裡面有两种不同的相关性,是我们需要產生两种不同的 head,来找两种不同的相关性</p>
<p>既然 q 有两个,那 k 也就要有两个,那 v 也就要有两个,从 q 得到 <span class="arithmatex">\(q^1 q^2\)</span>,从 k 得到 <span class="arithmatex">\(k^1 k^2\)</span>,从 v 得到 <span class="arithmatex">\(v^1 v^2\)</span>,那其实就是把 q 把 k 把 v,分别乘上两个矩阵,得到这个不同的 head,就这样子而已,</p>
<p>对另外一个位置,也做一样的事情</p>
<p>只是现在<span class="arithmatex">\(q^1\)</span>,它在算这个 attention 的分数的时候,它就不要管那个 <span class="arithmatex">\(k^2\)</span> 了</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210412194346086.png" alt="image-20210412194346086" style="zoom:50%;" /></p>
<ul>
<li>
<p>所以 <span class="arithmatex">\(q_{i,1}\)</span> 就跟 <span class="arithmatex">\(k^{i,1}\)</span> 算 attention</p>
</li>
<li>
<p><span class="arithmatex">\(q_{i,1}\)</span> 就跟 <span class="arithmatex">\(k^{j,1}\)</span> 算 attention,也就是算这个 dot product,然后得到这个 attention 的分数</p>
</li>
<li>然后今天在做 weighted sum 的时候,也不要管 <span class="arithmatex">\(v^2\)</span> 了,看 <span class="arithmatex">\(V^{i,1}\)</span> 跟 <span class="arithmatex">\(v^{j,1}\)</span> 就好,所以你把 attention 的分数乘 <span class="arithmatex">\(v^{i,1}\)</span>,把 attention 的分数乘 <span class="arithmatex">\(v^{j,1}\)</span></li>
<li>然后接下来就得到 <span class="arithmatex">\(b^{i,1}\)</span></li>
</ul>
<p>这边只用了其中一个 head,那你会用另外一个 head,也做一模一样的事情</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210412194533453.png" alt="image-20210412194533453" style="zoom:50%;" /></p>
<p>所以 <span class="arithmatex">\(q^2\)</span> 只对 <span class="arithmatex">\(k^2\)</span> 做 attention,它们在做 weighted sum 的时候,只对 <span class="arithmatex">\(v^2\)</span> 做 weighted sum,然后接下来你就得到 <span class="arithmatex">\(b^{i,2}\)</span></p>
<p>如果你有多个 head,有 8 个 head 有 16 个 head,那也是一样的操作,那这边是用两个 head 来当作例子,来给你看看有两个 head 的时候,是怎麼操作的,现在得到 <span class="arithmatex">\(b^{i,1}\)</span> 跟 <span class="arithmatex">\(b^{i,2}\)</span></p>
<p>然后接下来你可能会把 <span class="arithmatex">\(b^{i,1}\)</span> 跟 <span class="arithmatex">\(b^{i,2}\)</span>,把它接起来,然后再通过一个 transform</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210412194643379.png" alt="image-20210412194643379" style="zoom:67%;" /></p>
<p>也就是再乘上一个矩阵,然后得到 bi,然后再送到下一层去,那这个就是 Multi-head attention,一个这个 Self-attention 的变形</p>
<h2 id="positional-encoding">Positional Encoding<a class="headerlink" href="#positional-encoding" title="Permanent link">&para;</a></h2>
<h3 id="no-position-information-in-self-attention">No position information in self-attention<a class="headerlink" href="#no-position-information-in-self-attention" title="Permanent link">&para;</a></h3>
<p>那讲到目前為止,你会发现说 Self-attention 的这个 layer,它少了一个也许很重要的资讯,这个资讯是<strong>位置的资讯</strong></p>
<p>对一个 Self-attention layer 而言,每一个 input,它是出现在 sequence 的最前面,还是最后面,它是完全没有这个资讯的</p>
<p>对 Self-attention 而言,<strong>位置 1 跟位置 2 跟位置 3 跟位置 4,完全没有任何差别,这四个位置的操作其实是一模一样</strong>,对它来说 q1 到跟 q4 的距离,并没有特别远,1 跟 4 的距离并没有特别远,2 跟 3 的距离也没有特别近</p>
<p>对它来说就是天涯若比邻,所有的位置之间的距离都是一样的,没有任何一个位置距离比较远,也没有任何位置距离比较近,也没有谁在整个 sequence 的最前面,也没有谁在整个 sequence 的最后面</p>
<p>但是这样子设计可能会有一些问题,因為有时候位置的资讯也许很重要,举例来说,我们在做这个 POS tagging,就是词性标记的时候,也许你知道说<strong>动词比较不容易出现在句首</strong>,所以如果我们知道说,某一个词汇它是放在句首的,那它是动词的可能性可能就比较低,这样子的位置的资讯往往也是有用的</p>
<h3 id="each-positon-has-a-unique-positional-vector-ei">Each positon has a unique positional vector <span class="arithmatex">\(e^i\)</span><a class="headerlink" href="#each-positon-has-a-unique-positional-vector-ei" title="Permanent link">&para;</a></h3>
<p>可是在我们到目前為止,讲的 Self-attention 的操作裡面,根本就没有位置的资讯,所以怎麼办呢,所以你做 Self-attention 的时候,如果你觉得位置的资讯是一个重要的事情,那你可以把位置的资讯把它塞进去,怎麼把位置的资讯塞进去呢,这边就要用到一个叫做,==positional encoding== 的技术</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210412201736447.png" alt="image-20210412201736447" style="zoom:67%;" /></p>
<p>你為每一个位置设定一个 vector,叫做 positional vector,这边<strong>用 <span class="arithmatex">\(e^i\)</span> 来表示,上标 i 代表是位置,每一个不同的位置</strong>,就有不同的 vector,就是 <span class="arithmatex">\(e^1\)</span> 是一个 vector,<span class="arithmatex">\(e^2\)</span> 是一个vector,<span class="arithmatex">\(e^{128}\)</span> 是一个vector,不同的位置都有一个它专属的 e,然后把这个 e 加到 <span class="arithmatex">\(a^i\)</span> 上面,就结束了</p>
<p>就是告诉你的 Self-attention,位置的资讯,如果它看到说 <span class="arithmatex">\(a^i\)</span> 好像有被加上 $ e^i$,它就知道说现在出现的位置,应该是在 i 这个位置</p>
<p>最早的这个 transformer,就 Attention Is All You Need 那篇 paper 裡面,它用的 $ e^i$长的是这个样子</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210412201911959.png" alt="image-20210412201911959" style="zoom:50%;" /></p>
<p>这边这个图上面,每一个 column 就代表一个 e,第一个位置就是 <span class="arithmatex">\(e^1\)</span>,第二个位置就是 <span class="arithmatex">\(e^2\)</span>,第三个位置就是 <span class="arithmatex">\(e^3\)</span>,以此类推</p>
<p>所以它就是把这边这个向量,放在第一个位置,把这个向量加到第二个位置的 a上,把这个向量加到第三个位置的 a 上,以此类推,每一个位置都有一个专属的 e,希望透过给每一个位置不同的 e,你的 model 在处理这个 input 的时候,它可以知道现在的 input,它的位置的资讯是什麼样子</p>
<h3 id="hand-crafted-or-learned-from-data">Hand-crafted or Learned from data<a class="headerlink" href="#hand-crafted-or-learned-from-data" title="Permanent link">&para;</a></h3>
<p>这样子的 positional vector,它是 handcrafted 的,也就是它是人设的,那人设的这个 vector 有很多问题,就假设我现在在定这个 vector 的时候,只定到 128,那我现在 sequence 的长度,如果是 129 怎麼办呢</p>
<p>不过在最早的那个,Attention Is All You Need paper裡面,没有这个问题,<strong>它 vector 是透过某一个规则所產生的</strong>,透过一个很神奇的sin cos 的 function 所產生的</p>
<p>其实你不一定要这麼產生, <strong>positional encoding仍然是一个尚待研究的问题</strong>,你可以创造自己新的方法,或甚至 positional encoding,是可以根据资料学出来的</p>
<p>那有关 positional encoding,你可以再参考一下文献,这个是一个尚待研究的问题,比如说我这边引用了一篇,这个是去年放在 arxiv 上的论文,所以可以想见这其实都是很新的论文</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210412203355853.png" alt="image-20210412203355853" style="zoom:50%;" /></p>
<p>裡面就是比较了跟提出了,新的 positional encoding</p>
<ul>
<li>比如说这个是最早的 positional encoding,它是用一个神奇的 sin function 所產生的</li>
<li>那如果你的 positional encoding,你把 positional encoding 裡面的数值,当作 network 参数的一部分,直接 learn 出来,看起来是这个样子的,这个图是那个横著看的,它是横著看的,它是每一个 row,代表一个 position,好 所以这个是这个最原始的,用 sin function 產生的,这个是 learn 出来的</li>
<li>它裡面又有神奇的做法,比如说这个,这个是用 RNN 生出来的,positional encording 是用 RNN 出来的,这篇 paper 提出来的叫做 FLOATER,是用个神奇的 network 生出来的,</li>
</ul>
<p>总之你有各式各样不同的方法,来產生 positional encoding,那目前我们还不知道哪一种方法最好,这是一个尚待研究中的问题,所以你不用纠结说,為什麼 Sinusoidal 最好,<strong>你永远可以提出新的做法</strong></p>
<h2 id="applications">Applications …<a class="headerlink" href="#applications" title="Permanent link">&para;</a></h2>
<p>Self-attention 当然是用得很广,我们已经提过很多次 transformer 这个东西</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210412204520085.png" alt="image-20210412204520085" style="zoom: 67%;" /></p>
<p>那我们大家也都知道说,在 NLP 的领域有一个东西叫做 BERT,BERT 裡面也用到 Self-attention,所以 Self-attention 在 NLP 上面的应用,是大家都耳熟能详的</p>
<p>但 <strong>Self-attention,不是只能用在 NLP 相关的应用上,它还可以用在很多其他的问题上</strong>,</p>
<h3 id="self-attention-for-speech">Self-attention for Speech<a class="headerlink" href="#self-attention-for-speech" title="Permanent link">&para;</a></h3>
<p>比如说在做语音的时候,你也可以用 Self-attention,不过在做语音的时候,你可能会对 Self-attention,做一些小小的改动</p>
<p>因為一般语音的,如果你要把一段声音讯号,表示成一排向量的话,这排<strong>向量可能会非常地长</strong>,</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210412205436769.png" alt="image-20210412205436769" style="zoom:67%;" /></p>
<p>而每一个向量,其实只代表了 10 millisecond 的长度而已,所以如果今天是 1 秒鐘的声音讯号,它就有 100 个向量了,5 秒鐘的声音讯号,就 500 个向量了,你随便讲一句话,都是上千个向量了</p>
<p>所以一段声音讯号,你要描述它的时候,那个像这个 vector 的 sequence 它的长度是非常可观的,那可观的 sequence,可观的长度,会造成什麼问题呢</p>
<p>你想想看,我们今天在<strong>计算这个 attention matrix 的时候,它的 计算complexity 是长度的平方</strong></p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210412210111322.png" alt="image-20210412210111322" style="zoom:67%;" /></p>
<p>计算这个 attention matrix A′你需要做 L 乘以 L 次的 inner product,那如果这个 L 的值很大的话,它的计算量就很可观,你也需要很大的这个 memory,才能够把这个矩阵存下来</p>
<p>所以今天如果在做语音辨识的时候,一句话所產生的这个 attention matrix,可能会太大,大到你根本就不容易处理,不容易训练,所以怎麼办呢</p>
<p>在做语音的时候,有一招叫做 ==Truncated Self-attention==</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210412210322691.png" alt="image-20210412210322691" style="zoom:50%;" /></p>
<p>Truncated Self-attention 做的事情就是,我们今天在做 Self-attention 的时候,<strong>不要看一整句话,就我们就只看一个小的范围就好</strong></p>
<p>那至於<strong>这个范围应该要多大,那个是人设定的</strong></p>
<p>那為什麼我们知道说,今天在做语音辨识的时候,也许只需要看一个小的范围就好,那就是<strong>取决於你对这个问题的理解</strong>,也许我们要辨识这个位置有什麼样的<strong>phoneme</strong>,这个位置有什麼样的内容,我们并不需要看整句话,只要看这句话,跟它前后一定范围之内的资讯,其实就可以判断</p>
<p>所以如果在做 Self-attention 的时候,也许没有必要看过一整个句子,也许没有必要让 Self-attention 考虑一整个句子,也许只需要考虑一个小范围就好,这样就可以加快运算的速度，这个是 Truncated Self-attention,</p>
<h3 id="self-attention-for-image">Self-attention for Image<a class="headerlink" href="#self-attention-for-image" title="Permanent link">&para;</a></h3>
<p>那其实 Self-attention ,还可以被用在影像上,Self-attention</p>
<p>那到目前為止,我们在讲 Self-attention 的时候,我们都说 <strong>Self-attention 适用的范围是：输入是一个 vector set 的时候</strong></p>
<p>一张图片啊,我们把它看作是一个很长的向量,那<strong>其实一张图片,我们也可以换一个观点,把它看作是一个 vector 的 set</strong></p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210412214143979.png" alt="image-20210412214143979" style="zoom:67%;" /></p>
<p>这个是一个解析度 5 乘以 10 的图片,那这一张图片呢,可以看作是一个 tensor,这个 tensor 的大小是 5 乘以 10 乘以 3,3 代表 RGB 这 3 个 channel</p>
<p>你可以把每一个位置的 pixel,看作是一个三维的向量,所以<strong>每一个 pixel,其实就是一个三维的向量</strong>,那<strong>整张图片,其实就是 5 乘以 10 个向量的set</strong></p>
<p>所以我们其实可以换一个角度,影像这个东西,其实也是一个 vector set,它既然也是一个 vector set 的话,你完全可以用 Self-attention 来处理一张图片,那有没有人用 Self-attention 来处理一张图片呢,是有的</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210412214417887.png" alt="image-20210412214417887" style="zoom:50%;" /></p>
<p>那这边就举了两个例子,来给大家参考,那现在把 Self-attention 用在影像处理上,也不算是一个非常石破天惊的事情,</p>
<h4 id="self-attention-vs-cnn">Self-attention v.s. CNN<a class="headerlink" href="#self-attention-vs-cnn" title="Permanent link">&para;</a></h4>
<p>我们可以来比较一下,Self-attention 跟 CNN 之间,有什麼样的差异或者是关联性</p>
<p>如果我们今天,是用 Self-attention 来处理一张图片,代表说,假设这个是你要考虑的 pixel,那它產生 query,其他 pixel 產生 key,</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210412214915856.png" alt="image-20210412214915856" style="zoom:50%;" /></p>
<p>你今天在做 inner product 的时候,你考虑的不是一个小的receptive field的信息,而是整张影像的资讯</p>
<p>但是今天在做 CNN 的时候,,会画出一个 receptive field,每一个 filter,每一个 neural,只考虑 receptive field 范围裡面的资讯</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210412215451745.png" alt="image-20210412215451745" style="zoom:50%;" /></p>
<ul>
<li>
<p>所以如果我们比较 CNN 跟 Self-attention 的话,<strong>CNN 可以看作是一种简化版的 Self-attention</strong>，因為在做CNN的时候,我们只考虑 receptive field 裡面的资讯,而在做 Self-attention 的时候,我们是考虑整张图片的资讯,所以 CNN,是简化版的 Self-attention</p>
</li>
<li>
<p>或者是你可以反过来说,<strong>Self-attention 是一个复杂化的 CNN</strong></p>
</li>
</ul>
<p>在 CNN 裡面,我们要划定 receptive field,每一个 neural,只考虑 receptive field 裡面的资讯,而 <strong>receptive field 的范围跟大小,是人决定的,</strong></p>
<p>而对 Self-attention 而言,我们用 attention,去找出相关的 pixel,就好像是 <strong>receptive field 是自动被学出的</strong>,network 自己决定说,receptive field 的形状长什麼样子,network 自己决定说,以这个 pixel 為中心,哪些 pixel 是我们真正需要考虑的,那些 pixel 是相关的</p>
<p><strong>所以 receptive field 的范围,不再是人工划定,而是让机器自己学出来</strong></p>
<p>其实你可以读一篇 paper,叫做 On the Relationship,between Self-attention and Convolutional Layers</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210412215841085.png" alt="image-20210412215841085" style="zoom:50%;" /></p>
<p>在这篇 paper 裡面,会用数学的方式严谨的告诉你说,其实这个 <strong>CNN就是 Self-attention 的特例,Self-attention 只要设定合适的参数,它可以做到跟 CNN 一模一样的事情</strong></p>
<p>所以 self attention,是更 flexible 的 CNN,而 CNN 是有受限制的 Self-attention,Self-attention 只要透过某些设计,某些限制,它就会变成 CNN</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210412220020641.png" alt="image-20210412220020641" style="zoom:50%;" /></p>
<p>那这也不是很旧的 paper,你发现它放到网路上的时间呢,是 19 年的 11 月,所以你知道这些,我们今天上课裡面讲的东西,其实都是很新的资讯</p>
<p>既然Self-attention 比较 flexible,之前有讲说<strong>比较 flexible 的 model,比较需要更多的 data,如果你 data 不够,就有可能 overfitting</strong></p>
<p>而小的 model,而比较有限制的 model,它适合在 data 小的,少的时候,它可能比较不会 overfitting,那如果你这个限制设的好,也会有不错的结果</p>
<p>如果你今天用不同的 data 量,来训练 CNN 跟 Self-attention,你确实可以看到我刚才讲的现象</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210412220707729.png" alt="image-20210412220707729" style="zoom:50%;" /></p>
<p>那这个实验结果,来自於 An image is worth 16 乘以 16 的 words,这个是 Google 的 paper,它就是把这个 Self-attention,apply 在影像上面</p>
<p>那其实<strong>把一张影像呢,拆成 16 乘以 16 个 patch,它把每一个 patch想像成是一个 word</strong>,因為一般我们这个 Self-attention,比较常用在 NLP 上面,所以他就说,想像每一个 patch 其实就是一个 word,所以他就取了一个很 fancy 的 title,叫做一张图呢,值 16 乘以 16 个文字</p>
<p>横轴是训练的影像的量,那你发现说,对 Google 来说 用的,所谓的资料量比较少,也是你没有办法用的资料量啦这边有 10 个 million 就是,1000 万张图,是资料量比较小的 setting,然后资料量比较大的 setting 呢,有 3 亿张图片,在这个实验裡面呢,比较了 Self-attention 是浅蓝色的这一条线,跟 CNN 是深灰色的这条线</p>
<p>就会发现说,<strong>随著资料量越来越多,那 Self-attention 的结果就越来越好,最终在资料量最多的时候,Self-attention 可以超过 CNN,但在资料量少的时候,CNN 它是可以比 Self-attention,得到更好的结果的</strong></p>
<p>那為什麼会这样,你就可以从 CNN 跟 Self-attention,它们的弹性来加以解释</p>
<ul>
<li>Self-attention 它弹性比较大,所以需要比较多的训练资料,训练资料少的时候,就会 overfitting</li>
<li>而 CNN 它弹性比较小,在训练资料少的时候,结果比较好,但训练资料多的时候,它没有办法从更大量的训练资料得到好处</li>
</ul>
<p>所以这个就是 Self-attention 跟 CNN 的比较，那 Self-attention 跟 CNN,谁比较好呢,<strong>我应该选哪一个呢,事实上你也可以都用</strong>,在我们作业四裡面,如果你要做 strong baseline 的话,就特别给你一个提示,就是用 conformer,裡面就是有用到 Self-attention,也有用到 CNN</p>
<h3 id="self-attention-vs-rnn">Self-attention v.s. RNN<a class="headerlink" href="#self-attention-vs-rnn" title="Permanent link">&para;</a></h3>
<p>我们来比较一下,Self-attention 跟 RNN,RNN就是 recurrent neural network,这门课裡面现在就不会讲到 recurrent neural network,因為 recurrent neural network 的角色,很大一部分都可以用 Self-attention 来取代了,</p>
<p>但是 RNN 是什麼呢,假设你想知道的话,那这边很快地三言两语把它带过去,RNN 跟 Self-attention 一样,都是要处理 input 是一个 sequence 的状况</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210413093042847.png" alt="image-20210413093042847" style="zoom:50%;" /></p>
<p>在 RNN 裡面呢</p>
<ul>
<li>左边是你的 input sequence,你有一个 memory 的 vector</li>
<li>然后你有一个 RNN 的 block,这个 RNN 的 block 呢,它吃 memory 的 vector,吃第一个 input 的 vector</li>
<li>然后 output 一个东西,然后根据这个 output 的东西,我们通常叫做这个 hidden,这个 hidden 的 layer 的 output</li>
<li>然后通过这个 fully connected network,然后再去做你想要的 prediction</li>
</ul>
<p>接下来当sequence 裡面,第二个 vector 作為 input 的时候,也会把前一个时间点吐出来的东西,当做下一个时间点的输入,再丢进 RNN 裡面,然后再產生新的 vector,再拿去给 fully connected network</p>
<p>然后第三个 vector 进来的时候,你把第三个 vector 跟前一个时间点的输出,一起丢进 RNN,再產生新的输出,然后在第四个时间点</p>
<p>第四个 vector 输入的时候,把第四个 vector 跟前一个时间点,產生出来的输出,再一起做处理,得到新的输出,再通过 fully connected network 的 layer,这个就是 RNN</p>
<p>Recurrent Neural Network跟 Self-attention 做的事情其实也非常像,它们的 <strong>input 都是一个 vector sequence</strong></p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210413152809037.png" alt="image-20210413152809037" style="zoom:50%;" /></p>
<p>Self-attention output 是另外一个 vector sequence,这裡面的每一个 vector,都<strong>考虑了整个 input sequence 以后</strong>,再给 fully connected network 去做处理</p>
<p>那 RNN 呢,它也会 output 另外一群 vector,这另外一排 vector 也会给,fully connected network 做进一步的处理,那 Self-attention 跟 RNN 有什麼不同呢</p>
<p>当然一个非常显而易见的不同,你可能会说,这边的每一个 vector,它都考虑了整个 input 的 sequence,而 RNN 每一个 vector,只考虑了左边已经输入的 vector,它没有考虑右边的 vector,那这是一个很好的观察</p>
<p>但是 <strong>RNN 其实也可以是双向的</strong>,所以如果你 RNN 用双向的 RNN 的话,其实这边的每一个 hidden 的 output,每一个 memory 的 output,其实也可以看作是考虑了整个 input 的 sequence</p>
<p>但是假设我们把 RNN 的 output,跟 Self-attention 的 output 拿来做对比的话,就算你用 bidirectional 的 RNN,还是有一些差别的</p>
<ul>
<li>
<p>对 RNN 来说,假设最右边这个黄色的 vector,要考虑最左边的这个输入,那它必须要把最左边的输入存在 memory 裡面,然后接下来都不能够忘掉,一路带到最右边,才能够在最后一个时间点被考虑</p>
</li>
<li>
<p>但对 Self-attention 来说没有这个问题,它只要这边输出一个 query,这边输出一个 key,只要它们 match 得起来,天涯若比邻,你可以从非常远的 vector,在整个 sequence 上非常远的 vector,轻易地抽取资讯,所以这是 RNN 跟 Self-attention,一个不一样的地方</p>
</li>
</ul>
<p>还有另外一个更主要的不同是,RNN 今天在处理的时候, input 一排 sequence,output 一排 sequence 的时候,<strong>RNN 是没有办法平行化的</strong></p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210413153504431.png" alt="image-20210413153504431" style="zoom:50%;" /></p>
<p>RNN 它今天 input 一排是 vector,output 另外一排 vector 的时候,它没有办法一次处理,没有办法平行处理所有的 output</p>
<p>但 Self-attention 有一个优势,是它可以平行处理所有的输出,你今天 input 一排 vector,再 output 这四个 vector 的时候,<strong>这四个 vector 是平行產生的,并不需要等谁先运算完才把其他运算出来</strong>,output 的这个 vector,裡面的 output 这个 vector sequence 裡面,每一个 vector 都是同时產生出来的</p>
<p>所以在运算速度上,Self-attention 会比 RNN 更有效率</p>
<p>那你今天发现说,<strong>很多的应用都往往把 RNN 的架构,逐渐改成 Self-attention 的架构了</strong>,如果你想要更进一步了解,RNN 跟 Self-attention 的关係的话,你可以看下面这篇文章,Transformers are RNNs,裡面会告诉你说,Self-attention 你加上了什麼东西以后,其实它就变成了 RNN,发现说这也不是很旧的 paper,这个是去年的六月放到 arXiv 上</p>
<p>所以今天讲的都是一些很新的研究成果,那 RNN 的部分呢,我们这门课就不会提到,假设你对 RNN 有兴趣的话,以下是这一门课之前的上课录影,那 RNN 的部分,因為这一次不会讲到,所以特别有做了英文的版本,RNN 呢 是中文英文版本,都同时有放在 YouTube 上面</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210413153721866.png" alt="image-20210413153721866" style="zoom:50%;" /></p>
<h3 id="self-attention-for-graph">Self-attention for Graph<a class="headerlink" href="#self-attention-for-graph" title="Permanent link">&para;</a></h3>
<p>Graph 也可以看作是一堆 vector,那如果是一堆 vector,就可以用 Self-attention 来处理,所以 Self-attention 也可以用在 Graph 上面,但是当我们把 Self-attention,用在Graph 上面的时候,有什麼样特别的地方呢,、</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210413154403199.png" alt="image-20210413154403199" style="zoom:50%;" /></p>
<p>在 Graph 上面,每一个 node 可以表示成一个向量,但<strong>不只有 node 的资讯,还有 edge 的资讯</strong>,我们知道哪些 node 之间是有相连的,也就是哪些 node 是有关联的</p>
<p>我们知道哪些向量间是有关联,那之前我们在做 Self-attention 的时候,所谓的关联性是 network 自己找出来的,但是现在既然有了 Graph 的资讯,<strong>有了 edge 的资讯,那关联性也许就不需要透过机器自动找出来,这个图上面的 edge 已经暗示了我们,node 跟 node 之间的关联性</strong></p>
<p>所以今天当你把 Self-attention,用在 Graph 上面的时候,你有一个选择是你在做这个,Attention Matrix 计算的时候,你可以<strong>只计算有 edge 相连的 node 就好</strong></p>
<p>举例来说在这个图上,node 1 跟 node 8 有相连,那我们只需要计算 node 1 跟 node 8,这两个向量之间的 attention 的分数,那 1 跟 6 相连,所以只有 1 跟 6 之间,需要计算 attention 的分数,1 跟 5 有相连,所以只有 1 跟 5 需要计算 attention 的分数,2 跟 3 有相连,所以只有 2 跟 3 需要计算 attention 的分数,以此类推</p>
<p>那如果两个 node 之间没有相连,那其实很有可能就暗示我们,这两个 node 之间没有关係,<strong>既然没有关係,我们就不需要再去计算它的 attention score,直接把它设為 0 就好了</strong></p>
<p>因為这个 <strong>Graph 往往是人為根据某些 domain knowledge 建出来的</strong>,那 domain knowledge 告诉我们说,这两个向量彼此之间没有关联,我们就没有必要再用机器去学习这件事情</p>
<p>其实当我们把 Self-attention,按照我们这边讲的这种限制,用在 Graph 上面的时候,其实就是一种 Graph Neural Network,也就是一种 GNN</p>
<p>那我知道 GNN,现在也是一个很 fancy 的题目,那我不会说 Self-attention 就要囊括了,所有 GNN 的各种变形了,但把 Self-attention 用在 Graph 上面,是某一种类型的 Graph Neural Network,那这边呢,一样我们也没有办法细讲了,GNN 这边坑也是很深啊,这边水是很深,那就放一下助教之前上课的连结</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210413154823956.png" alt="image-20210413154823956" style="zoom:50%;" /></p>
<p>大概花了快三个小时,在讲 Graph Neural Network,而且其实还没有讲完,就告诉你说这个 Graph Neural Network,也是有非常深的技术,这边水也是很深,那这不是我们今天这一堂课可以讲的内容,好 </p>
<h2 id="more">More<a class="headerlink" href="#more" title="Permanent link">&para;</a></h2>
<p>其实Self-attention 有非常非常多的变形,你可以看一篇 paper 叫做,Long Range Arena,裡面比较了各种不同的 Self-attention 的变形</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210413155347467.png" alt="image-20210413155347467" style="zoom:50%;" /></p>
<p>因為 Self-attention 它最大的问题就是,<strong>它的运算量非常地大</strong>,所以怎麼样减少 Self-attention 的运算量,是一个未来的重点,可以看到这边有,各种各式各样 Self-attention 的变形</p>
<p>Self-attention 最早是,用在 Transformer 上面,所以很多人讲 Transformer 的时候,其实它指的就是这个 Self-attention,有人说广义的 Transformer,指的就是 Self-attention,那所以后来各式各样的,Self-attention 的变形都这样做,都叫做是什麼 former,比如说 Linformer Performer Reformer 等等,所以 Self-attention 的变形,现在都叫做 xxformer</p>
<p>那可以看到，往右代表它运算的速度,所以有很多各式各样新的 xxformer,它们的速度会比原来的 Transformer 快,但是快的速度带来的就是 performance 变差</p>
<p>这个纵轴代表是 performance,所以它们往往比原来的 Transformer,performance 差一点,但是速度会比较快</p>
<p>那到底什麼样的 Self-attention,才能够真的又快又好,这仍然是一个尚待研究的问题,如果你对 Self-attention,想要进一步研究的话,你还可以看一下,Efficient Transformers: A Survey 这篇 paper,裡面会跟你介绍,各式各样 Self-attention 的变形。</p>





                
              </article>
            </div>
          
          
        </div>
        
          <a href="#" class="md-top md-icon" data-md-component="top" hidden>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
            回到页面顶部
          </a>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2023 hlx
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    <a href="https://space.bilibili.com/387119156" target="_blank" rel="noopener" title="Bilibili | 少年寥若晨星" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M488.6 104.1c16.7 18.1 24.4 39.7 23.3 65.7v202.4c-.4 26.4-9.2 48.1-26.5 65.1-17.2 17-39.1 25.9-65.5 26.7H92.02c-26.45-.8-48.21-9.8-65.28-27.2C9.682 419.4.767 396.5 0 368.2V169.8c.767-26 9.682-47.6 26.74-65.7C43.81 87.75 65.57 78.77 92.02 78h29.38L96.05 52.19c-5.75-5.73-8.63-13-8.63-21.79 0-8.8 2.88-16.06 8.63-21.797C101.8 2.868 109.1 0 117.9 0s16.1 2.868 21.9 8.603L213.1 78h88l74.5-69.397C381.7 2.868 389.2 0 398 0c8.8 0 16.1 2.868 21.9 8.603 5.7 5.737 8.6 12.997 8.6 21.797 0 8.79-2.9 16.06-8.6 21.79L394.6 78h29.3c26.4.77 48 9.75 64.7 26.1zm-38.8 69.7c-.4-9.6-3.7-17.4-10.7-23.5-5.2-6.1-14-9.4-22.7-9.8H96.05c-9.59.4-17.45 3.7-23.58 9.8-6.14 6.1-9.4 13.9-9.78 23.5v194.4c0 9.2 3.26 17 9.78 23.5s14.38 9.8 23.58 9.8H416.4c9.2 0 17-3.3 23.3-9.8 6.3-6.5 9.7-14.3 10.1-23.5V173.8zm-264.3 42.7c6.3 6.3 9.7 14.1 10.1 23.2V273c-.4 9.2-3.7 16.9-9.8 23.2-6.2 6.3-14 9.5-23.6 9.5-9.6 0-17.5-3.2-23.6-9.5-6.1-6.3-9.4-14-9.8-23.2v-33.3c.4-9.1 3.8-16.9 10.1-23.2 6.3-6.3 13.2-9.6 23.3-10 9.2.4 17 3.7 23.3 10zm191.5 0c6.3 6.3 9.7 14.1 10.1 23.2V273c-.4 9.2-3.7 16.9-9.8 23.2-6.1 6.3-14 9.5-23.6 9.5-9.6 0-17.4-3.2-23.6-9.5-7-6.3-9.4-14-9.7-23.2v-33.3c.3-9.1 3.7-16.9 10-23.2 6.3-6.3 14.1-9.6 23.3-10 9.2.4 17 3.7 23.3 10z"/></svg>
    </a>
  
    
    
    <a href="https://github.com/a-hlx" target="_blank" rel="noopener" title="GitHub | a-hlx" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.2.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../../../..", "features": ["navigation.tabs", "navigation.top", "navigation.indexes", "navigation.expand", "search.suggest", "search.highlight"], "search": "../../../../assets/javascripts/workers/search.e5c33ebb.min.js", "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}}</script>
    
    
      <script src="../../../../assets/javascripts/bundle.51d95adb.min.js"></script>
      
        <script src="../../../../mkdocs/javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>